{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFB4bJ0volLx"
      },
      "source": [
        "LLM Fine-tuning (Vertex AI Edition)\n",
        "\n",
        "We will fine-tune a Large Language Model (LLM) using industry-standard tools and Google Cloud's Vertex AI platform. We will use Hugging Face's `transformers` and `datasets` libraries, and Vertex AI Experiments.\n",
        "\n",
        "**Goal**: Fine-tune a `Gemma 3` model to speak like a Pirate!\n",
        "\n",
        "**Tech Stack**:\n",
        "*   **Model**: [Google Gemma 3 1B](https://huggingface.co/google/gemma-3-1b-it)\n",
        "*   **Dataset**: [Pirate UltraChat](https://huggingface.co/datasets/winglian/pirate-ultrachat-10k)\n",
        "*   **Logging & Eval**: Google Cloud Vertex AI (Experiments & Gemini Judge)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cxQthKYvolLy"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install transformers datasets peft google-cloud-aiplatform matplotlib python-dotenv --quiet\n",
        "!pip install torch --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qg67uJ9QolLy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Load environment variables from Google Colab\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load environment variables from .env file\n",
        "#from dotenv import load_dotenv\n",
        "#load_dotenv()\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Vertex AI\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, SafetySetting\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "1DvZ52lnruER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Initialize Vertex AI (.env)\n",
        "#GCP_PROJECT_ID = os.getenv('GOOGLE_CLOUD_PROJECT')\n",
        "\n",
        "# TODO: Initialize Vertex AI (Colab)\n",
        "GCP_PROJECT_ID = userdata.get('GCP_PROJECT_ID')\n",
        "\n",
        "vertexai.init(project=GCP_PROJECT_ID, location=\"us-east4\")\n"
      ],
      "metadata": {
        "id": "Nh4fWhx1r1qU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C9rvGL0olLy",
        "outputId": "ba5be0d5-13a5-4215-c8c3-311863a2a74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using CUDA Device: Tesla T4  |  0.00GB / 14.74GB Total\n"
          ]
        }
      ],
      "source": [
        "def setup_device():\n",
        "    \"\"\"\n",
        "    Checks for available hardware (CUDA, MPS, or CPU),\n",
        "    prints the status, and returns the torch device.\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"‚úÖ Using CUDA Device: {torch.cuda.get_device_name(0)}  |  {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f}GB Total\")\n",
        "\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"üçé Using Apple Metal Performance Shaders (MPS)\")\n",
        "\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"üíª Using CPU\")\n",
        "\n",
        "    return device\n",
        "\n",
        "device = setup_device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7zaXqAVolLy"
      },
      "source": [
        "Please make sure that your device is not CPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zp8y2nUmolLy"
      },
      "source": [
        "## 1. Data Preparation\n",
        "\n",
        "We will use the `winglian/pirate-ultrachat-10k` dataset from Hugging Face. This dataset contains conversations re-written in a pirate style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05dBLF5UolLz"
      },
      "outputs": [],
      "source": [
        "# Load dataset from Hugging Face\n",
        "dataset_id = \"winglian/pirate-ultrachat-10k\"\n",
        "dataset = load_dataset(dataset_id, split=\"train[:2000]\") # Use a subset for speed\n",
        "\n",
        "print(f\"Loaded {len(dataset)} samples.\")\n",
        "print(\"Sample entry:\", dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9A_bfh0olLz"
      },
      "source": [
        "## 2. Model Setup (Gemma 3)\n",
        "\n",
        "We use `google/gemma-3-1b-it`, the smallest and most efficient version of Google's latest open model series. While a 270M parameter version doesn't exist, this 1B model is highly optimized and perfect for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY08H8M9olLz"
      },
      "outputs": [],
      "source": [
        "model_id = \"google/gemma-3-1b-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9cceef0",
        "outputId": "fb25c961-ce30-43e7-8824-e27ca33fa7a4"
      },
      "source": [
        "# Check the device associated with the model\n",
        "print(f\"Model default device: {model.device}\")\n",
        "print(f\"First parameter is on: {next(model.parameters()).device}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model default device: cuda:0\n",
            "First parameter is on: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpjedbrBolLz",
        "outputId": "82097479-436a-47e4-fbcf-1892a1be9d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens shape: torch.Size([1, 9])\n",
            "Output tokens shape: torch.Size([1, 26])\n",
            "\n",
            "Model Response: \n",
            "\n",
            "\n",
            "Rome.\n",
            "\n",
            "Is there anything else you'd like to know?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What's the capital of Italy?\"\n",
        "tokens = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "print(f'Input tokens shape: {tokens.shape}')\n",
        "\n",
        "outputs = model.generate(tokens, max_new_tokens=50)\n",
        "print(f'Output tokens shape: {outputs.shape}')\n",
        "\n",
        "# Decode only the new tokens (answer) by slicing the output tensor\n",
        "# tokens.shape[1] is the length of the input prompt\n",
        "response = tokenizer.decode(outputs[0][tokens.shape[1]:], skip_special_tokens=True)\n",
        "print(f'\\nModel Response: \\n{response}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuOaefOolLz"
      },
      "source": [
        "## 3. LoRA Configuration\n",
        "\n",
        "We use Low-Rank Adaptation (LoRA) to fine-tune efficiently. We rely on the `peft` library's defaults to target the standard attention projection layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCatNrKnolLz"
      },
      "outputs": [],
      "source": [
        "def apply_lora(model):\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\n",
        "            \"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "        ],\n",
        "        lora_dropout=0.05\n",
        "    )\n",
        "    return get_peft_model(model, lora_config)\n",
        "\n",
        "model = apply_lora(model)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJcugfhQolLz"
      },
      "source": [
        "## 4. Training Loop with Vertex AI Logging\n",
        "\n",
        "We replace Comet with Vertex AI Experiments for tracking metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNfpETw6olLz"
      },
      "outputs": [],
      "source": [
        "def train(model, dataset, tokenizer, max_steps=600, accumulation_steps=8, learning_rate=2e-4, eval_samples=5):\n",
        "    # Initialize Vertex AI Experiment\n",
        "    vertexai.init(experiment=\"pirate-finetune-lab\")\n",
        "    vertexai.start_run(\"run-gemma3-pirate-advanced\")\n",
        "\n",
        "    # Advanced Optimizer & Scheduler Setup\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Scheduler: Reduce LR when loss plateaus\n",
        "    # Check every 5 *effective* updates\n",
        "    eval_interval = accumulation_steps * 5\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Simple data collator/formatter\n",
        "    def collate_fn(batch):\n",
        "        prompts = []\n",
        "        for item in batch:\n",
        "             chat = item['messages']\n",
        "             text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
        "             prompts.append(text)\n",
        "        return tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "    data_iter = iter(dataloader)\n",
        "\n",
        "    losses = []\n",
        "    print(f\"Starting Training (Max Steps: {max_steps}, Accumulation: {accumulation_steps})...\")\n",
        "\n",
        "    for step in tqdm(range(max_steps * accumulation_steps)):\n",
        "        try:\n",
        "            batch = next(data_iter)\n",
        "        except StopIteration:\n",
        "            data_iter = iter(dataloader)\n",
        "            batch = next(data_iter)\n",
        "\n",
        "        outputs = model(**batch, labels=batch['input_ids'])\n",
        "        loss = outputs.loss / accumulation_steps\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # Track real (un-scaled) loss\n",
        "        losses.append(loss.item() * accumulation_steps)\n",
        "\n",
        "        # --- Optimizer Step ---\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # --- Scheduler & Logging Step ---\n",
        "        if (step + 1) % eval_interval == 0:\n",
        "            avg_loss = np.mean(losses)\n",
        "            scheduler.step(avg_loss)\n",
        "\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            print(f\"Step {(step+1)//accumulation_steps}: Loss {avg_loss:.4f} | LR {current_lr:.2e}\")\n",
        "\n",
        "            # Log to Vertex AI\n",
        "            vertexai.log_metrics({\n",
        "                \"loss\": avg_loss,\n",
        "                \"learning_rate\": current_lr\n",
        "            }, step=(step+1)//accumulation_steps)\n",
        "\n",
        "            losses = [] # Reset buffer\n",
        "\n",
        "    # --- Automated Evaluation Phase ---\n",
        "    print(\"\\nTraining Complete. Running Evaluation...\")\n",
        "    model.eval()\n",
        "    eval_scores = []\n",
        "\n",
        "    test_prompts = [\n",
        "        \"Tell me about your favorite treasure.\",\n",
        "        \"What should we do with the rum?\",\n",
        "        \"Where is the hidden island?\",\n",
        "        \"How do you handle a mutiny?\",\n",
        "        \"What is the best way to sail through a storm?\"\n",
        "    ]\n",
        "\n",
        "    for i in range(min(eval_samples, len(test_prompts))):\n",
        "        prompt = test_prompts[i]\n",
        "        response = generate_response(model, tokenizer, prompt)\n",
        "        score = evaluate_pirate_style(response)\n",
        "        eval_scores.append(score)\n",
        "        print(f\"Sample {i+1} | Prompt: {prompt} | Score: {score}/10\")\n",
        "\n",
        "    avg_pirate_score = np.mean(eval_scores)\n",
        "\n",
        "    vertexai.log_metrics({\"final_avg_pirate_score\": avg_pirate_score})\n",
        "\n",
        "    print(f\"\\nFinal Average Pirate Score: {avg_pirate_score:.2f}/10\")\n",
        "    print(\"View your results at: https://console.cloud.google.com/vertex-ai/experiments\")\n",
        "\n",
        "    vertexai.end_run()\n",
        "    return model\n",
        "\n",
        "# Start training and logging\n",
        "# model = train(model, dataset, tokenizer, max_steps=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdIketW6olLz"
      },
      "source": [
        "## 5. Evaluation with Gemini (LLM-as-a-Judge)\n",
        "\n",
        "We use Google's Gemini 1.5 Flash via Vertex AI to evaluate the \"Pirate-ness\" of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b08maCtolLz"
      },
      "outputs": [],
      "source": [
        "def evaluate_pirate_style(text):\n",
        "    judge_model = GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You are a strict judge evaluating if the following text sounds like a real pirate.\n",
        "    Text: \"{text}\"\n",
        "\n",
        "    Rate the 'Pirate Style' on a scale of 1-10.\n",
        "    Return ONLY a JSON object: {{'score': <number>}}\n",
        "    \"\"\"\n",
        "\n",
        "    response = judge_model.generate_content(prompt)\n",
        "    try:\n",
        "        # Clean up code blocks if Gemini returns them\n",
        "        content = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        return json.loads(content)['score']\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "# Test evaluation\n",
        "sample_text = \"Arr matey! Hand over yer loot or walk the plank!\"\n",
        "score = evaluate_pirate_style(sample_text)\n",
        "print(f\"Pirate Score: {score}/10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVGcFdEZolLz"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "You have successfully fine-tuned a model using standard tools and integrated it with Google Cloud Vertex AI for professional-grade logging and evaluation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Fine-tuning (Vertex AI Edition)\n",
    "\n",
    "We will fine-tune a Large Language Model (LLM) using industry-standard tools and Google Cloud's Vertex AI platform. We will use Hugging Face's `transformers` and `datasets` libraries, and Vertex AI Experiments.\n",
    "\n",
    "**Goal**: Fine-tune a `Gemma 3` model to speak like a Pirate!\n",
    "\n",
    "**Tech Stack**:\n",
    "*   **Model**: [Google Gemma 3 1B](https://huggingface.co/google/gemma-3-1b-it)\n",
    "*   **Dataset**: [Pirate UltraChat](https://huggingface.co/datasets/winglian/pirate-ultrachat-10k)\n",
    "*   **Logging & Eval**: Google Cloud Vertex AI (Experiments & Gemini Judge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers datasets peft google-cloud-aiplatform --quiet\n",
    "!pip install torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Vertex AI\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, SafetySetting\n",
    "\n",
    "# TODO: Initialize Vertex AI\n",
    "# vertexai.init(project=\"YOUR_PROJECT_ID\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "We will use the `winglian/pirate-ultrachat-10k` dataset from Hugging Face. This dataset contains conversations re-written in a pirate style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "dataset_id = \"winglian/pirate-ultrachat-10k\"\n",
    "dataset = load_dataset(dataset_id, split=\"train[:2000]\") # Use a subset for speed\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples.\")\n",
    "print(\"Sample entry:\", dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Setup (Gemma 3)\n",
    "\n",
    "We use `google/gemma-3-1b-it`, the smallest and most efficient version of Google's latest open model series. While a 270M parameter version doesn't exist, this 1B model is highly optimized and perfect for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LoRA Configuration\n",
    "\n",
    "We use Low-Rank Adaptation (LoRA) to fine-tune efficiently. We rely on the `peft` library's defaults to target the standard attention projection layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model):\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        lora_dropout=0.05\n",
    "    )\n",
    "    return get_peft_model(model, lora_config)\n",
    "\n",
    "model = apply_lora(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop with Vertex AI Logging\n",
    "\n",
    "We replace Comet with Vertex AI Experiments for tracking metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, tokenizer, max_steps=100):\n",
    "    # Initialize Vertex AI Experiment\n",
    "    vertexai.init(experiment=\"pirate-finetune-lab\")\n",
    "    vertexai.start_run(\"run-1\")\n",
    "    \n",
    "    optimizer = Lion(model.parameters(), lr=1e-4)\n",
    "    model.train()\n",
    "    \n",
    "    # Simple data collator/formatter\n",
    "    def collate_fn(batch):\n",
    "        prompts = []\n",
    "        for item in batch:\n",
    "             # Assuming dataset has 'messages' list with 'content'\n",
    "             # We adapt to a simple format for this lab\n",
    "             # Note: Adjust key names based on actual dataset inspection\n",
    "             chat = item['messages'] # Adapt this if dataset structure differs\n",
    "             # Construct simple prompt: User -> Assistant\n",
    "             text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False)\n",
    "             prompts.append(text)\n",
    "        \n",
    "        return tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    step = 0\n",
    "    losses = []\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        if step >= max_steps: break\n",
    "        \n",
    "        outputs = model(**batch, labels=batch['input_ids'])\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            avg_loss = np.mean(losses[-10:])\n",
    "            print(f\"Step {step}: Loss {avg_loss:.4f}\")\n",
    "            # Log to Vertex AI\n",
    "            vertexai.log_metrics({\"loss\": avg_loss}, step=step)\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "    vertexai.end_run()\n",
    "    return model\n",
    "\n",
    "# Start training\n",
    "# model = train(model, dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation with Gemini (LLM-as-a-Judge)\n",
    "\n",
    "We use Google's Gemini 1.5 Flash via Vertex AI to evaluate the \"Pirate-ness\" of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pirate_style(text):\n",
    "    judge_model = GenerativeModel(\"gemini-2.5-flash\")\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a strict judge evaluating if the following text sounds like a real pirate.\n",
    "    Text: \"{text}\"\n",
    "    \n",
    "    Rate the 'Pirate Style' on a scale of 1-10.\n",
    "    Return ONLY a JSON object: {{'score': <number>}}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = judge_model.generate_content(prompt)\n",
    "    try:\n",
    "        # Clean up code blocks if Gemini returns them\n",
    "        content = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        return json.loads(content)['score']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Test evaluation\n",
    "sample_text = \"Arr matey! Hand over yer loot or walk the plank!\"\n",
    "score = evaluate_pirate_style(sample_text)\n",
    "print(f\"Pirate Score: {score}/10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have successfully fine-tuned a model using standard tools and integrated it with Google Cloud Vertex AI for professional-grade logging and evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
